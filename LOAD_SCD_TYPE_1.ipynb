{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fe385f-a786-4a10-acee-32cd56a61979",
   "metadata": {},
   "source": [
    "IMPORT AND FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "39d7cbfc-64e0-4d13-9180-be0a89608dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import logging\n",
    "import pandas as pd\n",
    "import oracledb\n",
    "\n",
    "def setup_logging(config):\n",
    "    # Retrieve logging configuration from config\n",
    "    log_file = config.get('logging', 'log_file')\n",
    "    log_level = config.get('logging', 'log_level')\n",
    "\n",
    "    # Create directory for log file if it does not exist\n",
    "    log_dir = os.path.dirname(log_file)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Configure logging settings\n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        level=getattr(logging, log_level.upper()),\n",
    "        format='%(asctime)s:%(levelname)s:%(message)s'\n",
    "    )\n",
    "\n",
    "    # Set up console logging\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.ERROR)\n",
    "    formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "    console.setFormatter(formatter)\n",
    "    logging.getLogger('').addHandler(console)\n",
    "\n",
    "def fetch_data(user, password, dsn, table_name):\n",
    "    try:\n",
    "        # Construct the SQL query\n",
    "        query = f\"SELECT * FROM {user}.{table_name}\"\n",
    "        print(query)  # Debug print statements\n",
    "        print(user)\n",
    "        print(password)\n",
    "        print(dsn)\n",
    "\n",
    "        # Connect to the Oracle database\n",
    "        connection = oracledb.connect(user=user, password=password, dsn=dsn)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Execute the query and fetch data\n",
    "        cursor.execute(query)\n",
    "        columns = [col[0] for col in cursor.description]\n",
    "        data = cursor.fetchall()\n",
    "\n",
    "        # Close the cursor and connection\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "\n",
    "        # Convert data to a pandas DataFrame\n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        # Log any exceptions during data fetch\n",
    "        logging.error(f\"Error fetching data: {e}\")\n",
    "        raise e\n",
    "\n",
    "def upsert_to_customers(df, target_user, target_password, target_dsn, key_columns, source_suffix, target_suffix, table_name):\n",
    "    try:\n",
    "        source_len = len(source_suffix)\n",
    "        target_len = len(target_suffix)\n",
    "\n",
    "        # Connect to the target Oracle database\n",
    "        connection = oracledb.connect(user=target_user, password=target_password, dsn=target_dsn)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Iterate over DataFrame rows to perform upsert operations\n",
    "        for _, row in df.iterrows():\n",
    "            # Determine if the key columns indicate existing records in target\n",
    "            if not all([row[f\"{key}{target_suffix}_exists\"] for key in key_columns]):\n",
    "                # Insert new record if it does not exist\n",
    "                insert_columns = key_columns[:]\n",
    "                insert_values = [row[key] for key in key_columns]\n",
    "                placeholders = [f':{key}' for key in key_columns]\n",
    "\n",
    "                # Add non-key columns to the insertion list\n",
    "                for column in df.columns:\n",
    "                    if column.endswith(source_suffix) and column[:-source_len] not in key_columns:\n",
    "                        base_column = column[:-source_len]\n",
    "                        insert_columns.append(base_column)\n",
    "                        insert_values.append(row[column])\n",
    "                        placeholders.append(f\":{base_column}\")\n",
    "\n",
    "                # Construct and execute the insert statement\n",
    "                insert_query = f\"\"\"\n",
    "                    INSERT INTO {table_name} ({', '.join(insert_columns)})\n",
    "                    VALUES ({', '.join(placeholders)})\n",
    "                \"\"\"\n",
    "                insert_bind_dict = {base_column: value for base_column, value in zip(insert_columns, insert_values)}\n",
    "                cursor.execute(insert_query, insert_bind_dict)\n",
    "            else:\n",
    "                # Update existing record if it exists\n",
    "                update_columns = []\n",
    "                bind_dict = {key: row[key] for key in key_columns}\n",
    "\n",
    "                # Compare each column to check for updates\n",
    "                for column in df.columns:\n",
    "                    if column.endswith(source_suffix):\n",
    "                        base_column = column[:-source_len]\n",
    "                        target_column = base_column + target_suffix\n",
    "                        if target_column in df.columns:\n",
    "                            if row[column] != row[target_column]:\n",
    "                                update_columns.append(f\"{base_column} = :{base_column}\")\n",
    "                                bind_dict[base_column] = row[column]\n",
    "\n",
    "                # Construct and execute the update statement\n",
    "                if update_columns:\n",
    "                    key_condition = ' AND '.join([f\"{key} = :{key}\" for key in key_columns])\n",
    "                    update_query = f\"UPDATE {table_name} SET {', '.join(update_columns)} WHERE {key_condition}\"\n",
    "                    cursor.execute(update_query, bind_dict)\n",
    "\n",
    "        # Commit the transaction and close the connection\n",
    "        connection.commit()\n",
    "        connection.close()\n",
    "    except Exception as e:\n",
    "        # Log  any exceptions that occur during the upsert operation\n",
    "        logging.error(f\"Error during upsert operation: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847a96b-b294-473f-a191-fe53bb6c1624",
   "metadata": {},
   "source": [
    "MAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d0fd6ad-072c-47f4-91fd-bdbc882f22c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM etl.S_CUSTOMERS\n",
      "etl\n",
      "etl\n",
      "localhost:1521/XEPDB1\n",
      "SELECT * FROM app.CUSTOMERS\n",
      "app\n",
      "app\n",
      "localhost:1521/XEPDB1\n",
      "   CUSTOMER_ID EMAIL_ADDRESS_etl FULL_NAME_etl  REC_STS           REC_TMSTP  \\\n",
      "0          392             1qdqw         2ewew        1 2024-09-01 16:03:32   \n",
      "1         1666             3dqqw        dsdsds        1 2024-08-30 17:18:39   \n",
      "\n",
      "  EMAIL_ADDRESS_app FULL_NAME_app _merge  CUSTOMER_ID_app_exists  \n",
      "0             1qdqw         2ewew   both                    True  \n",
      "1             3dqqw        dsdsds   both                    True  \n"
     ]
    }
   ],
   "source": [
    "def main(env='production'):\n",
    "    try:\n",
    "        # Configuration Variables\n",
    "        #Specifies key columns for merging, table name for upsert operations, and suffixes for source and target data columns.\n",
    "        \n",
    "        key_columns = [\"CUSTOMER_ID\"]  # List of key columns for merging\n",
    "        table_name = \"CUSTOMERS\"  # Table name for upsert operations\n",
    "        source_suffix = \"_etl\"  # Suffix for source data columns\n",
    "        target_suffix = \"_app\"  # Suffix for target data columns\n",
    "        \n",
    "        # Load configuration from config file\n",
    "        # Loads the configuration file which contains database connection details and logging settings.\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('config/config.ini')\n",
    "\n",
    "        # Extract source and target sections from the configuration file\n",
    "        # Determines the configuration sections to use based on the environment (e.g., production).\n",
    "        source_section = f'{env}_etl'\n",
    "        target_section = f'{env}_app'\n",
    "\n",
    "        # Get database connection details # Extracts the details for connecting to the source and target databases.\n",
    "        source_user = config[source_section]['username']\n",
    "        source_password = config[source_section]['password']\n",
    "        source_dsn = config[source_section]['dsn']\n",
    "        target_user = config[target_section]['username']\n",
    "        target_password = config[target_section]['password']\n",
    "        target_dsn = config[target_section]['dsn']\n",
    "\n",
    "        # Set up logging\n",
    "        setup_logging(config)\n",
    "        logging.info(f'Starting the database operations script in {env} environment.')\n",
    "\n",
    "        # Fetch data from the source and target databases\n",
    "        source_data = fetch_data(source_user, source_password, source_dsn, \"S_\" + table_name)  # Added prefix S_ for staging table\n",
    "        target_data = fetch_data(target_user, target_password, target_dsn, table_name)\n",
    "\n",
    "        # Merge source and target data on key columns with indicator column to identify matched and unmatched records.\n",
    "        df_merged = source_data.merge(target_data, on=key_columns, how=\"left\", suffixes=(source_suffix, target_suffix), indicator=True)\n",
    "\n",
    "        # Add columns to detect if each key exists in both dataframes\n",
    "        for key in key_columns:\n",
    "            df_merged[key + target_suffix + '_exists'] = df_merged['_merge'] == 'both'\n",
    "\n",
    "        # Debug print to check merged dataframe\n",
    "        print(df_merged)\n",
    "        \n",
    "        # Perform the upsert operation (insert/update logic)\n",
    "        upsert_to_customers(df_merged, target_user, target_password, target_dsn, key_columns, source_suffix, target_suffix, table_name)\n",
    "\n",
    "        logging.info(f'Data synchronization between S_{table_name} and {table_name} completed.')\n",
    "    except Exception as e:\n",
    "        # Log any exceptions that occur during the main function execution\n",
    "        logging.error(f\"Error in main function: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Entry point for the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd75ce-c59a-4b4b-9f64-e856c03e63dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32231ba4-83a0-43a5-b187-7c4c4d661b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
